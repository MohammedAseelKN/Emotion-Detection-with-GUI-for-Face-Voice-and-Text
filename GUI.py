# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qUvm7KDOaOnj7GPqEZDjZvBNe19n-Gj3
"""

# Install required packages
!pip install -q deepface transformers librosa gradio numpy opencv-python

# Import libraries
import numpy as np
import cv2
from deepface import DeepFace
from transformers import pipeline
import librosa
import gradio as gr
import warnings
warnings.filterwarnings('ignore')

## 1. Facial Emotion Recognition (using DeepFace)
def analyze_face(image):
    try:
        # Convert Gradio Image to numpy array
        img = np.array(image)

        # Convert RGBA to RGB if needed
        if img.shape[2] == 4:
            img = img[:, :, :3]

        # Analyze face
        results = DeepFace.analyze(img, actions=['emotion'], enforce_detection=False)

        if isinstance(results, list):
            result = results[0]
        else:
            result = results

        # Draw bounding box
        if 'region' in result:
            x, y, w, h = result['region']['x'], result['region']['y'], result['region']['w'], result['region']['h']
            cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
            cv2.putText(img, result['dominant_emotion'], (x, y-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)

        return img, result['dominant_emotion'], result['emotion']
    except Exception as e:
        print(f"Face analysis error: {e}")
        return image, "Error", {}

## 2. Text Emotion Analysis (using Transformers)
try:
    text_analyzer = pipeline("text-classification", model="bhadresh-savani/distilbert-base-uncased-emotion")
except:
    text_analyzer = pipeline("text-classification")

def analyze_text(text):
    try:
        result = text_analyzer(text)[0]
        return result['label'], result['score']
    except Exception as e:
        print(f"Text analysis error: {e}")
        return "Error", 0.0

## 3. Voice Emotion Analysis (Rule-based)
def analyze_voice(audio_path):
    try:
        y, sr = librosa.load(audio_path, sr=16000)

        # Extract features
        pitch = np.mean(librosa.yin(y, fmin=50, fmax=2000))
        energy = np.sum(y**2) / len(y)
        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))

        # Simple rules
        if energy < 0.001:
            return "neutral", 0.9
        elif pitch > 500 and spectral_centroid > 2000:
            return "happy", 0.8
        elif pitch < 100 and spectral_centroid < 1000:
            return "sad", 0.7
        elif energy > 0.01 and spectral_centroid > 2500:
            return "angry", 0.75
        else:
            return "neutral", 0.6
    except Exception as e:
        print(f"Voice analysis error: {e}")
        return "Error", 0.0

## 4. Gradio Interface
def analyze_all(image=None, audio=None, text=None):
    results = {}

    # Process image
    if image is not None:
        img, dominant_emotion, emotion_details = analyze_face(image)
        results['Face'] = {
            'processed_image': img,
            'dominant_emotion': dominant_emotion,
            'emotion_details': emotion_details
        }

    # Process audio
    if audio is not None:
        emotion, confidence = analyze_voice(audio)
        results['Voice'] = {
            'emotion': emotion,
            'confidence': confidence
        }

    # Process text
    if text is not None and text.strip() != "":
        emotion, confidence = analyze_text(text)
        results['Text'] = {
            'emotion': emotion,
            'confidence': confidence
        }

    return results

# Create interface
with gr.Blocks() as demo:
    gr.Markdown("# Multi-Modal Emotion Detection System")

    with gr.Row():
        with gr.Column():
            image_input = gr.Image(label="Upload Face Image", type="pil")
            audio_input = gr.Audio(label="Record or Upload Voice", type="filepath")
            text_input = gr.Textbox(label="Enter Text for Analysis", lines=3)
            submit_btn = gr.Button("Analyze")

        with gr.Column():
            with gr.Tab("Face"):
                face_output_img = gr.Image(label="Processed Image")
                face_emotion = gr.Label(label="Dominant Emotion")
                face_details = gr.JSON(label="Emotion Details")

            with gr.Tab("Voice"):
                voice_emotion = gr.Label(label="Detected Emotion")
                voice_confidence = gr.Label(label="Confidence")

            with gr.Tab("Text"):
                text_emotion = gr.Label(label="Detected Emotion")
                text_confidence = gr.Label(label="Confidence")

    submit_btn.click(
        fn=analyze_all,
        inputs=[image_input, audio_input, text_input],
        outputs=[
            face_output_img, face_emotion, face_details,
            voice_emotion, voice_confidence,
            text_emotion, text_confidence
        ]
    )

# Launch
demo.launch()